---
title: "Research"
layout: single
showToc: false
hideMeta: true
---

                       
<section class="rk-tl">

  <!-- Ongoing -->
  <!--<div class="rk-group">
    <div class="rk-year">Ongoing</div>
    <div class="rk-items">

<article class="rk-item">
<div class="rk-left"><span class="rk-tag">aFed</span></div>
<div class="rk-body">
<h3 class="rk-title">Adaptive Federated Gaussian Process Regression via Two-Round Communication</h3>
<div class="rk-meta">
<strong>Niladri Kal</strong>, Botond SzabÃ³, Rajarshi Guhaniyogi, Natesh Pillai, Debdeep Pati 
<span class="rk-status"><em>In preparation</em></span>
</div>

<div class="rk-pills">
<details class="rk-details">
        <summary class="rk-pill">Awards</summary>
        <ul class="rk-awards"><li>ðŸ¥ˆ Silver Prize â€“ 2024 SETCASA Poster Session</li></ul>
    </details>
    </div>
    </div>
    </article>

</div>
</div>-->

  <!-- 2025 -->
  <div class="rk-group">
    <div class="rk-year">2025</div>
    <div class="rk-items">
<article class="rk-item">
<div class="rk-left"><span class="rk-tag">HierBOSSS</span></div>
<div class="rk-body">
<h3 class="rk-title">Hierarchical Bayesian Operator-induced Symbolic Regression Trees for Structural Learning of Scientific Expressions</h3>
<div class="rk-meta">
  <strong>Somjit Roy</strong>, Pritam Dey, Debdeep Pati, and Bani K. Mallick  
  <span class="rk-status"><em>Submitted, Under review</em></span>
</div>


<div class="rk-pills">
<details class="rk-details">
        <summary class="rk-pill">Awards</summary>
        <ul class="rk-awards"><li>2026 ASA SBSS Student Paper Award</li></ul>
        <ul class="rk-awards"><li>2026 ASA SDSS Refereed Talk</li></ul>
    </details>
    <details class="rk-details">
        <summary class="rk-pill">Abstract</summary>
        <div class="rk-abstract">
            The advent of Scientific Machine Learning has heralded a transformative era in scientific discovery, driving progress across diverse domains. Central to this progress is uncovering scientific laws from experimental data through symbolic regression. However, existing approaches are dominated by heuristic algorithms or data-hungry black-box methods, which often demand low-noise settings and lack principled uncertainty quantification. Motivated by interpretable Statistical Artificial Intelligence, we develop a hierarchical Bayesian framework for symbolic regression that represents scientific laws as ensembles of tree-structured symbolic expressions endowed with a regularized tree prior. This coherent probabilistic formulation enables full posterior inference via an efficient Markov chain Monte Carlo algorithm, yielding a balance between predictive accuracy and structural parsimony. To guide symbolic model selection, we develop a marginal posterior-based criterion adhering to the Occam's window principle and further quantify structural fidelity to ground truth through a tailored expression-distance metric. On the theoretical front, we establish near-minimax rate of Bayesian posterior concentration, providing the first rigorous guarantee in context of symbolic regression. Empirical evaluation demonstrates robust performance of our proposed methodology against state-of-the-art competing modules on a simulated example, a suite of canonical Feynman equations, and single-atom catalysis dataset.
        </div>
    </details>
    <a class="rk-pill rk-external" href="https://arxiv.org/abs/2509.19710"
    target="_blank" rel="noopener noreferrer">
    arXiv
    </a>
    </div>
    </div>
    </article>


<div class="rk-items">
<article class="rk-item">
<div class="rk-left"><span class="rk-tag">TAVIE-SSG</span></div>
<div class="rk-body">
<h3 class="rk-title">A Generalized Tangent Approximation based Variational Inference Framework for Strongly Super-Gaussian Likelihoods</h3>
<div class="rk-meta">
  <strong>Somjit Roy</strong>, Pritam Dey, Debdeep Pati, and Bani K. Mallick  
  <span class="rk-status"><em>Submitted, Under review</em></span>
</div>


<div class="rk-pills">
<!--<details class="rk-details">
        <summary class="rk-pill">Awards</summary>
        <ul class="rk-awards"><li>2026 ASA SBSS Student Paper Award</li></ul>
        <ul class="rk-awards"><li>2026 ASA SDSS Refereed Talk</li></ul>
    </details>-->
    <details class="rk-details">
        <summary class="rk-pill">Abstract</summary>
        <div class="rk-abstract">
            Variational inference, as an alternative to Markov chain Monte Carlo sampling, has played a transformative role in enabling scalable computation for complex Bayesian models. Nevertheless, existing approaches often depend on either rigid model-specific formulations or stochastic black-box optimization routines.  Tangent approximation is a principled class of structured variational methods that exploits the geometry of the underlying probability model. However, its utility has largely been confined to logistic regression and related modeling regimes. In this article, we propose a novel variational framework based on tangent transformation for a broad class of probability models characterized by strongly super-Gaussian likelihoods. Our method leverages convex duality to construct tangent minorants of the log-likelihood, thereby inducing conjugacy with Gaussian priors over model parameters in an otherwise intractable setup. Under mild assumptions on the data-generating mechanism, we establish algorithmic convergence guarantees, a contribution that stands in contrast to the limited theoretical assurances typically available for black-box variational methods. Additionally, we derive near-minimax optimal bounds for the variational risk. Superior performance of our proposed methodology is illustrated on simulated and real-data scenarios that challenge state-of-the-art variational algorithms in terms of scalability and their ability to consistently capture complex underlying data structure.
        </div>
    </details>
    <a class="rk-pill rk-external" href="https://arxiv.org/abs/2504.05431"
    target="_blank" rel="noopener noreferrer">
    arXiv
    </a>
    </div>
    </div>
    </article>
    

</div>
</div>

</section>
